{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcac8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import hdbscan\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d960db30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SpaCy loaded.\n",
      " 326 stopwords loaded.\n"
     ]
    }
   ],
   "source": [
    "experiential_terms = {\n",
    "    'emotional': [\n",
    "        'felt','feeling','emotion','joy','fear','anxiety','bliss','love','terror','peace','calm',\n",
    "        'excited','overwhelmed','gratitude','euphoria','sadness','longing','crying','ecstasy','relief',\n",
    "        'compassion','grief','awe','anger','release','hope','despair','serenity','agitation','comfort',\n",
    "        'purging','vulnerability','intimacy','empathy','tension','melancholy','abandon','appreciation'\n",
    "    ],\n",
    "    'sensory': [\n",
    "        'visual','hear','sound','color','bright','pattern','geometry','music','taste','smell','see',\n",
    "        'saw','colors','sounds','shapes','textures','movement','melting','vibrations','pulsing',\n",
    "        'fractal','echo','flashing','tunnel','fluid','shimmering','sparkling','synesthesia',\n",
    "        'auditory','trails','glow','hallucination','pulsate','distortion','radiance','static',\n",
    "        'blurred','lightness','glimmer','resonance','tactile','kaleidoscopic'\n",
    "    ],\n",
    "    'cognitive': [\n",
    "        'thought','mind','consciousness','aware','realize','understand','insight','clarity','confused',\n",
    "        'clear','thinking','perception','concepts','identity','ego','dissolve','looping','logic',\n",
    "        'recognition','belief','interpretation','memory','language','narrative','meaning','mindspace',\n",
    "        'headspace','overthinking','mental','clarification','self-talk','rational','intellect',\n",
    "        'philosophical','metacognition','rumination','stream of consciousness','inner dialogue',\n",
    "        'cognitive dissonance','hyperfocus'\n",
    "    ],\n",
    "    'physical': [\n",
    "        'body','skin','breath','heart','energy','vibration','tingling','warm','heavy','light','pressure',\n",
    "        'sensation','nausea','shaking','sweating','floating','stillness','tightness','spasm','motion',\n",
    "        'trembling','cold','breathing','heartbeat','twitching','dry mouth','muscles','stiffness',\n",
    "        'paralysis','numbness','restlessness','chills','sweat','clenching','somatic','bodyload',\n",
    "        'temperature','digestive','physical release'\n",
    "    ],\n",
    "    'mystical': [\n",
    "        'ego','self','unity','divine','spiritual','transcend','infinite','oneness','god','universe',\n",
    "        'connected','sacred','eternal','death','rebirth','timeless','interconnected','presence','source',\n",
    "        'void','light','beyond','higher power','awakening','realm','dimension','truth','immortality',\n",
    "        'ego death','no-self','nirvana','cosmic','transcendence','pure being','karma','light being',\n",
    "        'soul','heaven','angelic','time distortion','godlike','divinity','portal','third eye',\n",
    "        'nondual','dissolution','samsara','infinity','entity','timelessness'\n",
    "    ],\n",
    "    'temporal': [\n",
    "        'onset','peak','comedown','duration','timeline','hours','minutes','start','beginning',\n",
    "        'after','later','build-up','before','end','wave','early','gradual','suddenly',\n",
    "        'phase','stage','passed','elapsed','over time','rush','fade','linger','moment',\n",
    "        'slowly','time passed','time distorted','hour mark','entry','exit'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten to lowercase set\n",
    "EXP_TERMS_FLAT = {word.lower() for words in experiential_terms.values() for word in words}\n",
    "\n",
    "# Load SpaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\", \"lemmatizer\", \"ner\"])\n",
    "    if \"senter\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe(\"sentencizer\")\n",
    "    print(\" SpaCy loaded.\")\n",
    "except OSError:\n",
    "    raise RuntimeError(\"SpaCy model not found. Run: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "SPACY_STOPWORDS = {word.lower() for word in SPACY_STOPWORDS}\n",
    "print(f\" {len(SPACY_STOPWORDS)} stopwords loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54ef39ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_whitespace_re = re.compile(r\"[ \\t\\v\\f]+\")\n",
    "_newlines_re   = re.compile(r\"\\s*\\n\\s*\")\n",
    "\n",
    "def clean_text_basic(txt: str) -> str:\n",
    "    if not isinstance(txt, str) or pd.isna(txt):\n",
    "        return \"\"\n",
    "    txt = txt.lower()\n",
    "    txt = txt.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    txt = _newlines_re.sub(\"\\n\", txt)\n",
    "    txt = _whitespace_re.sub(\" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "def sentencize_and_clean(raw_text: str, nlp):\n",
    "    raw_text_clean = clean_text_basic(raw_text)\n",
    "    doc = nlp.make_doc(raw_text_clean)\n",
    "    if \"senter\" in nlp.pipe_names:\n",
    "        nlp.get_pipe(\"senter\")(doc)\n",
    "    else:\n",
    "        doc = nlp(raw_text_clean)\n",
    "    sents = [s.text.strip() for s in doc.sents if s.text.strip()]\n",
    "    return sents, raw_text_clean\n",
    "\n",
    "\n",
    "_word_re_cache = {}\n",
    "def _has_term(sentence: str, term: str) -> bool:\n",
    "    rx = _word_re_cache.get(term)\n",
    "    if rx is None:\n",
    "        term_rx = r\"\\b\" + r\"\\s+\".join(map(re.escape, term.split())) + r\"\\b\"\n",
    "        rx = re.compile(term_rx)\n",
    "        _word_re_cache[term] = rx\n",
    "    return rx.search(sentence) is not None\n",
    "\n",
    "def _exp_terms_in(sentences):\n",
    "    present = set()\n",
    "    for t in EXP_TERMS_FLAT:\n",
    "        if any(_has_term(s, t) for s in sentences):\n",
    "            present.add(t)\n",
    "    return present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f5cd246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_hdbscan_summary(\n",
    "    sentences,\n",
    "    n_components: int = 24,\n",
    "    min_cluster_size: int = 7,\n",
    "    min_samples: int = 1,\n",
    "    top_k_ratio: float = 0.25,\n",
    "    max_clusters: int = 2,\n",
    "    pos_bias: float = 0.0\n",
    ") -> str:\n",
    "    if not sentences:\n",
    "        return \"\"\n",
    "    sents = [s for s in sentences if s.strip()]\n",
    "    if len(sents) < 2:\n",
    "        return \" \".join(sents)\n",
    "\n",
    "    # TF-IDF + LSA\n",
    "    tfidf = TfidfVectorizer(lowercase=False, stop_words=\"english\", max_features=2000, sublinear_tf=True, norm=\"l2\")\n",
    "    X_tfidf = tfidf.fit_transform(sents)\n",
    "    n_features = X_tfidf.shape[1]\n",
    "\n",
    "    max_rank = max(2, min(n_components, n_features - 1, len(sents) - 1, 64))\n",
    "    if max_rank < 2:\n",
    "        k = max(1, int(round(len(sents) * top_k_ratio)))\n",
    "        return \" \".join(sents[:k])\n",
    "\n",
    "    svd = TruncatedSVD(n_components=max_rank, random_state=42)\n",
    "    X_lsa = svd.fit_transform(X_tfidf)\n",
    "    X_lsa = X_lsa / (np.linalg.norm(X_lsa, axis=1, keepdims=True) + 1e-10)\n",
    "\n",
    "    # HDBSCAN Clustering\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom',\n",
    "        prediction_data=True\n",
    "    )\n",
    "    labels = clusterer.fit_predict(X_lsa)\n",
    "\n",
    "    unique_labels = np.setdiff1d(np.unique(labels), [-1])\n",
    "    if len(unique_labels) == 0:\n",
    "        k = max(1, int(round(len(sents) * top_k_ratio)))\n",
    "        return \" \".join(sents[:k])\n",
    "\n",
    "    # Cluster scoring\n",
    "    try:\n",
    "        probs = clusterer.probabilities_\n",
    "    except Exception:\n",
    "        probs = np.ones(len(sents))\n",
    "\n",
    "    doc_exp_terms = _exp_terms_in(sents)\n",
    "    cluster_scores = []\n",
    "\n",
    "    for lbl in unique_labels:\n",
    "        idxs = np.where(labels == lbl)[0]\n",
    "        size = len(idxs)\n",
    "        if size == 0:\n",
    "            continue\n",
    "        prob_mean = float(np.mean(probs[idxs]))\n",
    "        sents_in_cluster = [sents[i] for i in idxs]\n",
    "        exp_in_cluster = _exp_terms_in(sents_in_cluster)\n",
    "        exp_coverage = (len(exp_in_cluster & doc_exp_terms) / max(1, len(doc_exp_terms))) if doc_exp_terms else 1.0\n",
    "        score = (size**0.7) * (prob_mean**1.2) * (0.6 + 0.4*exp_coverage)\n",
    "        cluster_scores.append((score, idxs))\n",
    "\n",
    "    # Select top clusters\n",
    "    cluster_scores.sort(key=lambda z: z[0], reverse=True)\n",
    "    cluster_scores = cluster_scores[:max_clusters]\n",
    "\n",
    "    # Allocate sentences proportionally\n",
    "    k_final = max(1, int(round(len(sents) * top_k_ratio)))\n",
    "    if not cluster_scores:\n",
    "        return \" \".join(sents[:k_final])\n",
    "\n",
    "    weights = np.array([s for s, _ in cluster_scores], dtype=float)\n",
    "    weights = weights / (weights.sum() + 1e-9)\n",
    "    quotas = np.maximum(1, np.round(weights * k_final)).astype(int)\n",
    "    while quotas.sum() > k_final:\n",
    "        quotas[np.argmax(quotas)] -= 1\n",
    "    while quotas.sum() < k_final:\n",
    "        quotas[np.argmin(quotas)] += 1\n",
    "\n",
    "    selected = []\n",
    "    for (score, idxs), q in zip(cluster_scores, quotas):\n",
    "        centroid = np.mean(X_lsa[idxs], axis=0)\n",
    "        centroid /= (np.linalg.norm(centroid) + 1e-10)\n",
    "        sims = cosine_similarity(X_lsa[idxs], centroid.reshape(1, -1)).ravel()\n",
    "        sims = sims + pos_bias * (1.0 - (idxs / (len(sents) + 1e-9)))\n",
    "        order = idxs[np.argsort(-sims)]\n",
    "        selected.extend(order[:q])\n",
    "\n",
    "    # Deduplicate & sort by original position\n",
    "    selected = sorted(dict.fromkeys(selected))\n",
    "    return \" \".join(sents[i] for i in selected[:k_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a98110a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tfidf_cosine(a: str, b: str) -> float:\n",
    "    if not a.strip() or not b.strip():\n",
    "        return 0.0\n",
    "    vec = TfidfVectorizer(lowercase=False, stop_words=\"english\", max_features=4000)\n",
    "    try:\n",
    "        X = vec.fit_transform([a, b])\n",
    "        return float(cosine_similarity(X[0], X[1])[0,0])\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def score_semantic(summary: str, document: str) -> float:\n",
    "    return _tfidf_cosine(summary, document)\n",
    "\n",
    "def score_experiential(summary: str, document: str) -> float:\n",
    "    if not summary or not document:\n",
    "        return 0.0\n",
    "    doc_sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', document) if s.strip()]\n",
    "    sum_sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', summary) if s.strip()]\n",
    "    doc_terms = _exp_terms_in(doc_sents)\n",
    "    if not doc_terms:\n",
    "        return 1.0\n",
    "    sum_terms = _exp_terms_in(sum_sents)\n",
    "    return len(doc_terms & sum_terms) / len(doc_terms)\n",
    "\n",
    "def score_coherence(summary: str) -> float:\n",
    "    if not summary.strip():\n",
    "        return 0.0\n",
    "    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', summary) if s.strip()]\n",
    "    if len(sents) < 2:\n",
    "        return 1.0\n",
    "    vec = TfidfVectorizer(lowercase=False, stop_words=\"english\", max_features=4000)\n",
    "    try:\n",
    "        X = vec.fit_transform(sents)\n",
    "        sims = [float(cosine_similarity(X[i], X[i+1])[0,0]) for i in range(len(sents)-1)]\n",
    "        return float(np.mean(sims)) if sims else 0.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def custom_score(summary: str, document: str) -> float:\n",
    "    if not summary or not document:\n",
    "        return 0.0\n",
    "    sem = score_semantic(summary, document)\n",
    "    exp = score_experiential(summary, document)\n",
    "    coh = score_coherence(summary)\n",
    "    return 0.5*sem + 0.3*exp + 0.2*coh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "681dde15",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    \"n_components\": 24,\n",
    "    \"min_cluster_size\": 7,\n",
    "    \"min_samples\": 1,\n",
    "    \"top_k_ratio\": 0.25,\n",
    "    \"pos_bias\": 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954236b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing DMT...\n",
      "Loaded 100 reports for DMT\n",
      "Saved summaries to D:\\GitHub\\Psychedelics Summary\\Dataset\\Train Test\\dmt_test_100_with_summary.csv\n",
      " DMT - Avg Final Score: 0.375\n",
      "\n",
      " Processing LSD...\n",
      "Loaded 100 reports for LSD\n",
      "Saved summaries to D:\\GitHub\\Psychedelics Summary\\Dataset\\Train Test\\lsd_test_100_with_summary.csv\n",
      " LSD - Avg Final Score: 0.402\n",
      "\n",
      " Processing Psilocybin...\n",
      "Loaded 100 reports for Psilocybin\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(r\"\")\n",
    "test_files = {\n",
    "    \"DMT\": DATA_DIR / \"dmt_test_100.csv\",\n",
    "    \"LSD\": DATA_DIR / \"lsd_test_100.csv\",\n",
    "    \"Psilocybin\": DATA_DIR / \"mushroom_test_100.csv\"\n",
    "}\n",
    "\n",
    "def _norm_sub(x):\n",
    "    if not isinstance(x, str): return \"OTHER\"\n",
    "    y = x.strip().upper()\n",
    "    if y in {\"DMT\"}: return \"DMT\"\n",
    "    if y in {\"LSD\", \"ACID\"}: return \"LSD\"\n",
    "    if y in {\"PSILOCYBIN\", \"PSILOCYBIN MUSHROOM\", \"MUSHROOM\", \"MUSHROOMS\", \"PSILOCYBE\"}:\n",
    "        return \"Psilocybin\"\n",
    "    return y\n",
    "\n",
    "results = []\n",
    "\n",
    "for substance, file_path in test_files.items():\n",
    "    print(f\"\\n Processing {substance}...\")\n",
    "    if not file_path.exists():\n",
    "        print(f\" File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    if \"report_text\" not in df.columns:\n",
    "        raise KeyError(f\"Missing 'report_text' in {file_path}\")\n",
    "\n",
    "    df[\"_subst_norm\"] = df[\"substance\"].map(_norm_sub)\n",
    "    df = df[df[\"_subst_norm\"] == substance].copy()\n",
    "    print(f\"Loaded {len(df)} reports for {substance}\")\n",
    "\n",
    "    # Precompute sentences\n",
    "    cache = []\n",
    "    for text in df[\"report_text\"].astype(str).fillna(\"\").tolist():\n",
    "        sents, cleaned = sentencize_and_clean(text, nlp)\n",
    "        cache.append({\"sentences\": sents, \"doc_clean\": cleaned})\n",
    "\n",
    "    # Generate summaries\n",
    "    summaries = []\n",
    "    semantic_scores = []\n",
    "    experiential_scores = []\n",
    "    coherence_scores = []\n",
    "    final_scores = []\n",
    "\n",
    "    for entry in cache:\n",
    "        summary = lsa_hdbscan_summary(entry[\"sentences\"], **best_params)\n",
    "        summaries.append(summary)\n",
    "\n",
    "        sem = score_semantic(summary, entry[\"doc_clean\"])\n",
    "        exp = score_experiential(summary, entry[\"doc_clean\"])\n",
    "        coh = score_coherence(summary)\n",
    "        final = custom_score(summary, entry[\"doc_clean\"])\n",
    "\n",
    "        semantic_scores.append(sem)\n",
    "        experiential_scores.append(exp)\n",
    "        coherence_scores.append(coh)\n",
    "        final_scores.append(final)\n",
    "\n",
    "    # Add summary column\n",
    "    df[\"summary\"] = summaries\n",
    "    output_path = file_path.with_name(file_path.stem + \"_with_summary.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Saved summaries to {output_path}\")\n",
    "\n",
    "    # Aggregate scores\n",
    "    avg_sem = np.mean(semantic_scores)\n",
    "    avg_exp = np.mean(experiential_scores)\n",
    "    avg_coh = np.mean(coherence_scores)\n",
    "    avg_final = np.mean(final_scores)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": \"M2\",\n",
    "        \"Substance\": substance,\n",
    "        \"Semantic (TF-IDF/SBERT)\": f\"{avg_sem:.2f} (TF-IDF)\",\n",
    "        \"Experiential\": f\"{avg_exp:.2f}\",\n",
    "        \"Coherence (TF-IDF/SBERT)\": f\"{avg_coh:.2f} (TF-IDF)\",\n",
    "        \"Final Score\": f\"{avg_final:.2f}\"\n",
    "    })\n",
    "\n",
    "    print(f\" {substance} - Avg Final Score: {avg_final:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6610182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\" FINAL RESULTS: LSA+HDBSCAN (M2) ON TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6ced68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "results_df.to_csv(DATA_DIR / \"lsa_hdbscan_test_scores.csv\", index=False)\n",
    "print(f\"\\n Results saved to {DATA_DIR / 'lsa_hdbscan_test_scores.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef88b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
