{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ec1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) IMPORTS ===\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import hdbscan\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.pruners import MedianPruner\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740e2568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SpaCy loaded.\n",
      " 326 stopwords loaded.\n"
     ]
    }
   ],
   "source": [
    "# === 2) EXPERIENTIAL TERMS & SPACY SETUP ===\n",
    "\n",
    "experiential_terms = {\n",
    "    'emotional': [\n",
    "        'felt','feeling','emotion','joy','fear','anxiety','bliss','love','terror','peace','calm',\n",
    "        'excited','overwhelmed','gratitude','euphoria','sadness','longing','crying','ecstasy','relief',\n",
    "        'compassion','grief','awe','anger','release','hope','despair','serenity','agitation','comfort',\n",
    "        'purging','vulnerability','intimacy','empathy','tension','melancholy','abandon','appreciation'\n",
    "    ],\n",
    "    'sensory': [\n",
    "        'visual','hear','sound','color','bright','pattern','geometry','music','taste','smell','see',\n",
    "        'saw','colors','sounds','shapes','textures','movement','melting','vibrations','pulsing',\n",
    "        'fractal','echo','flashing','tunnel','fluid','shimmering','sparkling','synesthesia',\n",
    "        'auditory','trails','glow','hallucination','pulsate','distortion','radiance','static',\n",
    "        'blurred','lightness','glimmer','resonance','tactile','kaleidoscopic'\n",
    "    ],\n",
    "    'cognitive': [\n",
    "        'thought','mind','consciousness','aware','realize','understand','insight','clarity','confused',\n",
    "        'clear','thinking','perception','concepts','identity','ego','dissolve','looping','logic',\n",
    "        'recognition','belief','interpretation','memory','language','narrative','meaning','mindspace',\n",
    "        'headspace','overthinking','mental','clarification','self-talk','rational','intellect',\n",
    "        'philosophical','metacognition','rumination','stream of consciousness','inner dialogue',\n",
    "        'cognitive dissonance','hyperfocus'\n",
    "    ],\n",
    "    'physical': [\n",
    "        'body','skin','breath','heart','energy','vibration','tingling','warm','heavy','light','pressure',\n",
    "        'sensation','nausea','shaking','sweating','floating','stillness','tightness','spasm','motion',\n",
    "        'trembling','cold','breathing','heartbeat','twitching','dry mouth','muscles','stiffness',\n",
    "        'paralysis','numbness','restlessness','chills','sweat','clenching','somatic','bodyload',\n",
    "        'temperature','digestive','physical release'\n",
    "    ],\n",
    "    'mystical': [\n",
    "        'ego','self','unity','divine','spiritual','transcend','infinite','oneness','god','universe',\n",
    "        'connected','sacred','eternal','death','rebirth','timeless','interconnected','presence','source',\n",
    "        'void','light','beyond','higher power','awakening','realm','dimension','truth','immortality',\n",
    "        'ego death','no-self','nirvana','cosmic','transcendence','pure being','karma','light being',\n",
    "        'soul','heaven','angelic','time distortion','godlike','divinity','portal','third eye',\n",
    "        'nondual','dissolution','samsara','infinity','entity','timelessness'\n",
    "    ],\n",
    "    'temporal': [\n",
    "        'onset','peak','comedown','duration','timeline','hours','minutes','start','beginning',\n",
    "        'after','later','build-up','before','end','wave','early','gradual','suddenly',\n",
    "        'phase','stage','passed','elapsed','over time','rush','fade','linger','moment',\n",
    "        'slowly','time passed','time distorted','hour mark','entry','exit'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten to lowercase set\n",
    "EXP_TERMS_FLAT = {word.lower() for words in experiential_terms.values() for word in words}\n",
    "\n",
    "# Load SpaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"tagger\", \"lemmatizer\", \"ner\"])\n",
    "    if \"senter\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe(\"sentencizer\")\n",
    "    print(\" SpaCy loaded.\")\n",
    "except OSError:\n",
    "    raise RuntimeError(\"SpaCy model not found. Run: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "# Stopwords (lowercase)\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS\n",
    "SPACY_STOPWORDS = {word.lower() for word in SPACY_STOPWORDS}\n",
    "print(f\" {len(SPACY_STOPWORDS)} stopwords loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65b4df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "_whitespace_re = re.compile(r\"[ \\t\\v\\f]+\")\n",
    "_newlines_re   = re.compile(r\"\\s*\\n\\s*\")\n",
    "\n",
    "def clean_text_basic(txt: str) -> str:\n",
    "    if not isinstance(txt, str) or pd.isna(txt):\n",
    "        return \"\"\n",
    "    txt = txt.lower()\n",
    "    txt = txt.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    txt = _newlines_re.sub(\"\\n\", txt)\n",
    "    txt = _whitespace_re.sub(\" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "def sentencize_and_clean(raw_text: str, nlp):\n",
    "    raw_text_clean = clean_text_basic(raw_text)\n",
    "    doc = nlp.make_doc(raw_text_clean)\n",
    "    if \"senter\" in nlp.pipe_names:\n",
    "        nlp.get_pipe(\"senter\")(doc)\n",
    "    else:\n",
    "        doc = nlp(raw_text_clean)\n",
    "    sents = [s.text.strip() for s in doc.sents if s.text.strip()]\n",
    "    return sents, raw_text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3891431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Balanced subset: (300, 8)\n",
      "_subst_norm\n",
      "Psilocybin    100\n",
      "LSD           100\n",
      "DMT           100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = Path(r\"final_train_900.csv\")\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing: {DATA_PATH}\")\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "if not {\"report_text\", \"substance\"}.issubset(df.columns):\n",
    "    raise KeyError(\"CSV must have 'report_text' and 'substance' columns\")\n",
    "\n",
    "def _norm_sub(x):\n",
    "    if not isinstance(x, str): return \"OTHER\"\n",
    "    y = x.strip().upper()\n",
    "    if y in {\"DMT\"}: return \"DMT\"\n",
    "    if y in {\"LSD\", \"ACID\"}: return \"LSD\"\n",
    "    if y in {\"PSILOCYBIN\", \"PSILOCYBIN MUSHROOM\", \"MUSHROOM\", \"MUSHROOMS\", \"PSILOCYBE\"}:\n",
    "        return \"Psilocybin\"\n",
    "    return y\n",
    "\n",
    "df[\"_subst_norm\"] = df[\"substance\"].map(_norm_sub)\n",
    "TARGETS = [\"DMT\", \"LSD\", \"Psilocybin\"]\n",
    "N_PER_CLASS = 100\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "balanced_dfs = []\n",
    "for s in TARGETS:\n",
    "    pool = df[df[\"_subst_norm\"] == s]\n",
    "    n_pick = min(N_PER_CLASS, len(pool))\n",
    "    if n_pick < N_PER_CLASS:\n",
    "        print(f\" Only {n_pick} available for {s}\")\n",
    "    balanced_dfs.append(pool.sample(n=n_pick, random_state=RANDOM_SEED))\n",
    "\n",
    "df_sub = pd.concat(balanced_dfs, ignore_index=True).sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "print(f\" Balanced subset: {df_sub.shape}\")\n",
    "print(df_sub['_subst_norm'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69f107a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Pre-splitting sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:16<00:00, 17.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Precomputed: 300 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\" Pre-splitting sentences...\")\n",
    "lsa_cache = []\n",
    "for text in tqdm(df_sub[\"report_text\"].astype(str).fillna(\"\"), total=len(df_sub)):\n",
    "    sentences, cleaned = sentencize_and_clean(text, nlp)\n",
    "    lsa_cache.append({\n",
    "        \"sentences\": sentences,\n",
    "        \"doc_clean\": cleaned\n",
    "    })\n",
    "print(f\" Precomputed: {len(lsa_cache)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d446359",
   "metadata": {},
   "outputs": [],
   "source": [
    "_word_re_cache = {}\n",
    "def _has_term(sentence: str, term: str) -> bool:  # CHANGED\n",
    "    rx = _word_re_cache.get(term)\n",
    "    if rx is None:\n",
    "        # Build a word-boundary regex for phrases (e.g., \"ego death\")\n",
    "        term_rx = r\"\\b\" + r\"\\s+\".join(map(re.escape, term.split())) + r\"\\b\"\n",
    "        rx = re.compile(term_rx)\n",
    "        _word_re_cache[term] = rx\n",
    "    return rx.search(sentence) is not None\n",
    "\n",
    "def _exp_terms_in(sentences):  # CHANGED\n",
    "    present = set()\n",
    "    for t in EXP_TERMS_FLAT:\n",
    "        if any(_has_term(s, t) for s in sentences):\n",
    "            present.add(t)\n",
    "    return present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7bd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsa_hdbscan_summary(\n",
    "    sentences,\n",
    "    n_components: int = 48,            # CHANGED (safer default)\n",
    "    min_cluster_size: int = 5,\n",
    "    min_samples: int = 3,\n",
    "    top_k_ratio: float = 0.20,         # CHANGED (closer to LexRank-like length)\n",
    "    max_clusters: int = 2,\n",
    "    pos_bias: float = 0.05             # CHANGED (small lead bias helps coherence)\n",
    ") -> str:\n",
    "    if not sentences:\n",
    "        return \"\"\n",
    "    sents = [s for s in sentences if s.strip()]\n",
    "    if len(sents) < 2:\n",
    "        return \" \".join(sents)\n",
    "\n",
    "    # TF-IDF + safer LSA rank (per-doc)\n",
    "    tfidf = TfidfVectorizer(\n",
    "        lowercase=False, stop_words=\"english\",\n",
    "        max_features=2000, sublinear_tf=True, norm=\"l2\"  # CHANGED\n",
    "    )\n",
    "    X_tfidf = tfidf.fit_transform(sents)\n",
    "    n_features = X_tfidf.shape[1]\n",
    "\n",
    "    max_rank = max(2, min(n_components, n_features - 1, len(sents) - 1, 64))  # CHANGED\n",
    "    if max_rank < 2:\n",
    "        k = max(1, int(round(len(sents) * top_k_ratio)))\n",
    "        return \" \".join(sents[:k])\n",
    "\n",
    "    svd = TruncatedSVD(n_components=max_rank, random_state=42)\n",
    "    X_lsa = svd.fit_transform(X_tfidf)\n",
    "    X_lsa = X_lsa / (np.linalg.norm(X_lsa, axis=1, keepdims=True) + 1e-10)\n",
    "\n",
    "    # HDBSCAN (euclidean on unit vectors ~ cosine)\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom',\n",
    "        prediction_data=True\n",
    "    )\n",
    "    labels = clusterer.fit_predict(X_lsa)\n",
    "\n",
    "    unique_labels = np.setdiff1d(np.unique(labels), [-1])\n",
    "    if len(unique_labels) == 0:\n",
    "        k = max(1, int(round(len(sents) * top_k_ratio)))\n",
    "        return \" \".join(sents[:k])\n",
    "\n",
    "    # Density proxy: membership probabilities (robust across versions)\n",
    "    try:                                           # CHANGED\n",
    "        probs = clusterer.probabilities_\n",
    "    except Exception:\n",
    "        probs = np.ones(len(sents))\n",
    "\n",
    "    # Experiential coverage (phrase-aware) within doc and cluster\n",
    "    doc_exp_terms = _exp_terms_in(sents)          # CHANGED\n",
    "\n",
    "    cluster_scores = []\n",
    "    for lbl in unique_labels:\n",
    "        idxs = np.where(labels == lbl)[0]\n",
    "        size = len(idxs)\n",
    "        if size == 0:\n",
    "            continue\n",
    "        prob_mean = float(np.mean(probs[idxs]))   #  dense core (CHANGED)\n",
    "        sents_in_cluster = [sents[i] for i in idxs]\n",
    "        exp_in_cluster = _exp_terms_in(sents_in_cluster)\n",
    "        exp_coverage = (len(exp_in_cluster & doc_exp_terms) / max(1, len(doc_exp_terms))) if doc_exp_terms else 1.0\n",
    "        # Reward big, dense, experientially rich clusters (CHANGED)\n",
    "        score = (size**0.7) * (prob_mean**1.2) * (0.6 + 0.4*exp_coverage)\n",
    "        cluster_scores.append((score, idxs))\n",
    "\n",
    "    # Pick top clusters\n",
    "    cluster_scores.sort(key=lambda z: z[0], reverse=True)\n",
    "    cluster_scores = cluster_scores[:max_clusters]\n",
    "\n",
    "    # Allocate sentences proportional to cluster weights (not equal split. CHANGED)\n",
    "    k_final = max(1, int(round(len(sents) * top_k_ratio)))\n",
    "    weights = np.array([s for s, _ in cluster_scores], dtype=float)\n",
    "    weights = weights / (weights.sum() + 1e-9)\n",
    "    quotas = np.maximum(1, np.round(weights * k_final)).astype(int)\n",
    "    while quotas.sum() > k_final:\n",
    "        i = np.argmax(quotas)\n",
    "        quotas[i] -= 1\n",
    "    while quotas.sum() < k_final:\n",
    "        i = np.argmin(quotas)\n",
    "        quotas[i] += 1\n",
    "\n",
    "    selected = []\n",
    "    for (score, idxs), q in zip(cluster_scores, quotas):\n",
    "        centroid = np.mean(X_lsa[idxs], axis=0)\n",
    "        centroid /= (np.linalg.norm(centroid) + 1e-10)\n",
    "        sims = cosine_similarity(X_lsa[idxs], centroid.reshape(1, -1)).ravel()\n",
    "\n",
    "        # small positional bias toward earlier sentences (CHANGED)\n",
    "        sims = sims + pos_bias * (1.0 - (idxs / (len(sents) + 1e-9)))\n",
    "\n",
    "        order = idxs[np.argsort(-sims)]\n",
    "        selected.extend(order[:q])\n",
    "\n",
    "    # De-dup & restore narrative order\n",
    "    selected = sorted(dict.fromkeys(selected))\n",
    "    return \" \".join(sents[i] for i in selected[:k_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30128426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tfidf_cosine(a: str, b: str) -> float:\n",
    "    if not a.strip() or not b.strip():\n",
    "        return 0.0\n",
    "    vec = TfidfVectorizer(lowercase=False, stop_words=\"english\", max_features=4000)\n",
    "    try:\n",
    "        X = vec.fit_transform([a, b])\n",
    "        return float(cosine_similarity(X[0], X[1])[0,0])\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def score_semantic(summary: str, document: str) -> float:\n",
    "    return _tfidf_cosine(summary, document)\n",
    "\n",
    "def score_experiential(summary: str, document: str) -> float:  # CHANGED\n",
    "    if not summary or not document:\n",
    "        return 0.0\n",
    "    # Sentence split for phrase-aware matching\n",
    "    doc_sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', document) if s.strip()]\n",
    "    sum_sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', summary) if s.strip()]\n",
    "    doc_terms = _exp_terms_in(doc_sents)\n",
    "    if not doc_terms:\n",
    "        return 1.0\n",
    "    sum_terms = _exp_terms_in(sum_sents)\n",
    "    return len(doc_terms & sum_terms) / len(doc_terms)\n",
    "\n",
    "def score_coherence(summary: str) -> float:\n",
    "    if not summary.strip():\n",
    "        return 0.0\n",
    "    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', summary) if s.strip()]\n",
    "    if len(sents) < 2:\n",
    "        return 1.0\n",
    "    vec = TfidfVectorizer(lowercase=False, stop_words=\"english\", max_features=4000)\n",
    "    try:\n",
    "        X = vec.fit_transform(sents)\n",
    "        sims = [float(cosine_similarity(X[i], X[i+1])[0,0]) for i in range(len(sents)-1)]\n",
    "        return float(np.mean(sims)) if sims else 0.0\n",
    "    except Exception:\n",
    "        return 0.0\n",
    "\n",
    "def custom_score(summary: str, document: str) -> float:\n",
    "    if not summary or not document:\n",
    "        return 0.0\n",
    "    sem = score_semantic(summary, document)\n",
    "    exp = score_experiential(summary, document)\n",
    "    coh = score_coherence(summary)\n",
    "    return 0.5*sem + 0.3*exp + 0.2*coh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15ed2581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 13:26:10,937] A new study created in memory with name: LSA_HDBSCAN_Tuned\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Tuning on 300 docs | Pruning every 25 docs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d543d929a16545ceb91a94cb6935940a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 13:26:42,439] Trial 0 finished with value: 0.3700002794852972 and parameters: {'n_components': 24, 'min_cluster_size': 8, 'min_samples': 1, 'top_k_ratio': 0.21, 'pos_bias': 0.04}. Best is trial 0 with value: 0.3700002794852972.\n",
      "[I 2025-08-11 13:27:09,421] Trial 1 finished with value: 0.3110802654771703 and parameters: {'n_components': 24, 'min_cluster_size': 3, 'min_samples': 4, 'top_k_ratio': 0.16999999999999998, 'pos_bias': 0.0}. Best is trial 0 with value: 0.3700002794852972.\n",
      "[I 2025-08-11 13:27:39,005] Trial 2 finished with value: 0.374389693791553 and parameters: {'n_components': 64, 'min_cluster_size': 4, 'min_samples': 1, 'top_k_ratio': 0.25, 'pos_bias': 0.0}. Best is trial 2 with value: 0.374389693791553.\n",
      "[I 2025-08-11 13:28:06,250] Trial 3 finished with value: 0.3632859978833547 and parameters: {'n_components': 64, 'min_cluster_size': 7, 'min_samples': 1, 'top_k_ratio': 0.22999999999999998, 'pos_bias': 0.06}. Best is trial 2 with value: 0.374389693791553.\n",
      "[I 2025-08-11 13:28:33,393] Trial 4 finished with value: 0.3482969461589353 and parameters: {'n_components': 48, 'min_cluster_size': 7, 'min_samples': 1, 'top_k_ratio': 0.21, 'pos_bias': 0.06}. Best is trial 2 with value: 0.374389693791553.\n",
      "[I 2025-08-11 13:29:06,595] Trial 5 finished with value: 0.4000945871596785 and parameters: {'n_components': 24, 'min_cluster_size': 6, 'min_samples': 1, 'top_k_ratio': 0.25, 'pos_bias': 0.08}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:29:34,320] Trial 6 finished with value: 0.31463036309876435 and parameters: {'n_components': 48, 'min_cluster_size': 4, 'min_samples': 2, 'top_k_ratio': 0.16999999999999998, 'pos_bias': 0.0}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:30:06,738] Trial 7 finished with value: 0.3612551285773192 and parameters: {'n_components': 24, 'min_cluster_size': 3, 'min_samples': 2, 'top_k_ratio': 0.25, 'pos_bias': 0.02}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:30:37,284] Trial 8 finished with value: 0.3521969847095919 and parameters: {'n_components': 32, 'min_cluster_size': 4, 'min_samples': 2, 'top_k_ratio': 0.21, 'pos_bias': 0.06}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:31:06,859] Trial 9 finished with value: 0.34926638772800384 and parameters: {'n_components': 64, 'min_cluster_size': 4, 'min_samples': 1, 'top_k_ratio': 0.21, 'pos_bias': 0.02}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:31:08,404] Trial 10 pruned. \n",
      "[I 2025-08-11 13:31:38,142] Trial 11 finished with value: 0.38379396300436924 and parameters: {'n_components': 16, 'min_cluster_size': 6, 'min_samples': 3, 'top_k_ratio': 0.25, 'pos_bias': 0.08}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:32:08,145] Trial 12 finished with value: 0.38379396300436924 and parameters: {'n_components': 16, 'min_cluster_size': 6, 'min_samples': 3, 'top_k_ratio': 0.25, 'pos_bias': 0.08}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:32:38,296] Trial 13 finished with value: 0.37004136533495957 and parameters: {'n_components': 16, 'min_cluster_size': 5, 'min_samples': 3, 'top_k_ratio': 0.22999999999999998, 'pos_bias': 0.08}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:32:50,604] Trial 14 pruned. \n",
      "[I 2025-08-11 13:32:52,251] Trial 15 pruned. \n",
      "[I 2025-08-11 13:32:54,011] Trial 16 pruned. \n",
      "[I 2025-08-11 13:33:24,365] Trial 17 finished with value: 0.37004136533495957 and parameters: {'n_components': 16, 'min_cluster_size': 5, 'min_samples': 3, 'top_k_ratio': 0.22999999999999998, 'pos_bias': 0.08}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:33:50,299] Trial 18 finished with value: 0.3745559825926018 and parameters: {'n_components': 24, 'min_cluster_size': 8, 'min_samples': 3, 'top_k_ratio': 0.25, 'pos_bias': 0.06}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:33:52,581] Trial 19 pruned. \n",
      "[I 2025-08-11 13:33:54,750] Trial 20 pruned. \n",
      "[I 2025-08-11 13:34:24,942] Trial 21 finished with value: 0.38379396300436924 and parameters: {'n_components': 16, 'min_cluster_size': 6, 'min_samples': 3, 'top_k_ratio': 0.25, 'pos_bias': 0.08}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:34:56,155] Trial 22 finished with value: 0.38379396300436924 and parameters: {'n_components': 16, 'min_cluster_size': 6, 'min_samples': 3, 'top_k_ratio': 0.25, 'pos_bias': 0.08}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:34:58,920] Trial 23 pruned. \n",
      "[I 2025-08-11 13:35:31,274] Trial 24 finished with value: 0.38379396300436924 and parameters: {'n_components': 16, 'min_cluster_size': 6, 'min_samples': 3, 'top_k_ratio': 0.25, 'pos_bias': 0.08}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:35:33,903] Trial 25 pruned. \n",
      "[I 2025-08-11 13:35:36,499] Trial 26 pruned. \n",
      "[I 2025-08-11 13:35:38,727] Trial 27 pruned. \n",
      "[I 2025-08-11 13:36:11,400] Trial 28 finished with value: 0.38773719845035043 and parameters: {'n_components': 24, 'min_cluster_size': 8, 'min_samples': 1, 'top_k_ratio': 0.22999999999999998, 'pos_bias': 0.06}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:36:42,932] Trial 29 finished with value: 0.37020252284694993 and parameters: {'n_components': 24, 'min_cluster_size': 8, 'min_samples': 1, 'top_k_ratio': 0.21, 'pos_bias': 0.06}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:37:14,298] Trial 30 finished with value: 0.3878760224065563 and parameters: {'n_components': 24, 'min_cluster_size': 8, 'min_samples': 1, 'top_k_ratio': 0.22999999999999998, 'pos_bias': 0.04}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:37:46,158] Trial 31 finished with value: 0.3878760224065563 and parameters: {'n_components': 24, 'min_cluster_size': 8, 'min_samples': 1, 'top_k_ratio': 0.22999999999999998, 'pos_bias': 0.04}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:38:18,086] Trial 32 finished with value: 0.3878760224065563 and parameters: {'n_components': 24, 'min_cluster_size': 8, 'min_samples': 1, 'top_k_ratio': 0.22999999999999998, 'pos_bias': 0.04}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:38:34,616] Trial 33 pruned. \n",
      "[I 2025-08-11 13:39:06,286] Trial 34 finished with value: 0.3874924471841112 and parameters: {'n_components': 24, 'min_cluster_size': 8, 'min_samples': 1, 'top_k_ratio': 0.22999999999999998, 'pos_bias': 0.02}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:39:09,039] Trial 35 pruned. \n",
      "[I 2025-08-11 13:39:25,028] Trial 36 pruned. \n",
      "[I 2025-08-11 13:39:57,122] Trial 37 finished with value: 0.38885569030101536 and parameters: {'n_components': 24, 'min_cluster_size': 7, 'min_samples': 1, 'top_k_ratio': 0.22999999999999998, 'pos_bias': 0.02}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:40:29,669] Trial 38 finished with value: 0.38823763122990956 and parameters: {'n_components': 24, 'min_cluster_size': 7, 'min_samples': 1, 'top_k_ratio': 0.22999999999999998, 'pos_bias': 0.0}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:40:31,850] Trial 39 pruned. \n",
      "[I 2025-08-11 13:40:34,544] Trial 40 pruned. \n",
      "[I 2025-08-11 13:41:06,821] Trial 41 finished with value: 0.38885569030101536 and parameters: {'n_components': 24, 'min_cluster_size': 7, 'min_samples': 1, 'top_k_ratio': 0.22999999999999998, 'pos_bias': 0.02}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:41:39,203] Trial 42 finished with value: 0.38885569030101536 and parameters: {'n_components': 24, 'min_cluster_size': 7, 'min_samples': 1, 'top_k_ratio': 0.22999999999999998, 'pos_bias': 0.02}. Best is trial 5 with value: 0.4000945871596785.\n",
      "[I 2025-08-11 13:41:41,992] Trial 43 pruned. \n",
      "[I 2025-08-11 13:41:44,463] Trial 44 pruned. \n",
      "[I 2025-08-11 13:42:01,094] Trial 45 pruned. \n",
      "[I 2025-08-11 13:42:02,920] Trial 46 pruned. \n",
      "[I 2025-08-11 13:42:35,821] Trial 47 finished with value: 0.40289293550093647 and parameters: {'n_components': 24, 'min_cluster_size': 7, 'min_samples': 1, 'top_k_ratio': 0.25, 'pos_bias': 0.0}. Best is trial 47 with value: 0.40289293550093647.\n",
      "[I 2025-08-11 13:43:06,368] Trial 48 finished with value: 0.394596988780306 and parameters: {'n_components': 24, 'min_cluster_size': 7, 'min_samples': 2, 'top_k_ratio': 0.25, 'pos_bias': 0.02}. Best is trial 47 with value: 0.40289293550093647.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 13:43:08,815] Trial 49 pruned. \n",
      "[I 2025-08-11 13:43:11,341] Trial 50 pruned. \n",
      "[I 2025-08-11 13:43:42,181] Trial 51 finished with value: 0.394596988780306 and parameters: {'n_components': 24, 'min_cluster_size': 7, 'min_samples': 2, 'top_k_ratio': 0.25, 'pos_bias': 0.02}. Best is trial 47 with value: 0.40289293550093647.\n",
      "[I 2025-08-11 13:44:13,594] Trial 52 finished with value: 0.394596988780306 and parameters: {'n_components': 24, 'min_cluster_size': 7, 'min_samples': 2, 'top_k_ratio': 0.25, 'pos_bias': 0.02}. Best is trial 47 with value: 0.40289293550093647.\n",
      "[I 2025-08-11 13:44:46,458] Trial 53 finished with value: 0.394596988780306 and parameters: {'n_components': 24, 'min_cluster_size': 7, 'min_samples': 2, 'top_k_ratio': 0.25, 'pos_bias': 0.02}. Best is trial 47 with value: 0.40289293550093647.\n",
      "[I 2025-08-11 13:44:49,517] Trial 54 pruned. \n",
      "[I 2025-08-11 13:45:21,304] Trial 55 finished with value: 0.394596988780306 and parameters: {'n_components': 24, 'min_cluster_size': 7, 'min_samples': 2, 'top_k_ratio': 0.25, 'pos_bias': 0.02}. Best is trial 47 with value: 0.40289293550093647.\n",
      "[I 2025-08-11 13:45:23,400] Trial 56 pruned. \n",
      "[I 2025-08-11 13:45:54,122] Trial 57 finished with value: 0.394596988780306 and parameters: {'n_components': 24, 'min_cluster_size': 7, 'min_samples': 2, 'top_k_ratio': 0.25, 'pos_bias': 0.02}. Best is trial 47 with value: 0.40289293550093647.\n",
      "[I 2025-08-11 13:45:56,437] Trial 58 pruned. \n",
      "[I 2025-08-11 13:46:27,239] Trial 59 finished with value: 0.3943023216189195 and parameters: {'n_components': 24, 'min_cluster_size': 7, 'min_samples': 2, 'top_k_ratio': 0.25, 'pos_bias': 0.0}. Best is trial 47 with value: 0.40289293550093647.\n",
      " Best Params:\n",
      "{\n",
      "  \"n_components\": 24,\n",
      "  \"min_cluster_size\": 7,\n",
      "  \"min_samples\": 1,\n",
      "  \"top_k_ratio\": 0.25,\n",
      "  \"pos_bias\": 0.0\n",
      "}\n",
      "Best Custom Score: 0.4029\n"
     ]
    }
   ],
   "source": [
    "N_COMP_CHOICES     = [16, 24, 32, 40, 48, 64]          # CHANGED\n",
    "MIN_SIZE_CHOICES   = [3, 4, 5, 6, 7, 8]                # CHANGED\n",
    "MIN_SAMPLE_CHOICES = [1, 2, 3, 4, 5]                   # CHANGED\n",
    "\n",
    "EVAL_INTERVAL = max(10, len(lsa_cache) // 12)\n",
    "print(f\"üîç Tuning on {len(lsa_cache)} docs | Pruning every {EVAL_INTERVAL} docs\")\n",
    "\n",
    "def objective(trial):\n",
    "    n_components   = trial.suggest_categorical(\"n_components\", N_COMP_CHOICES)\n",
    "    min_cluster_sz = trial.suggest_categorical(\"min_cluster_size\", MIN_SIZE_CHOICES)\n",
    "    min_samples    = trial.suggest_categorical(\"min_samples\", MIN_SAMPLE_CHOICES)\n",
    "    top_k_ratio    = trial.suggest_float(\"top_k_ratio\", 0.15, 0.25, step=0.02)  # CHANGED\n",
    "    pos_bias       = trial.suggest_float(\"pos_bias\", 0.0, 0.08, step=0.02)      # CHANGED\n",
    "\n",
    "    scores = []\n",
    "    for i, entry in enumerate(lsa_cache, 1):\n",
    "        summary = lsa_hdbscan_summary(\n",
    "            sentences=entry[\"sentences\"],\n",
    "            n_components=n_components,\n",
    "            min_cluster_size=min_cluster_sz,\n",
    "            min_samples=min_samples,\n",
    "            top_k_ratio=top_k_ratio,\n",
    "            max_clusters=2,\n",
    "            pos_bias=pos_bias\n",
    "        )\n",
    "        score = custom_score(summary, entry[\"doc_clean\"])\n",
    "        scores.append(score)\n",
    "\n",
    "        # MedianPruner heartbeat\n",
    "        if i % EVAL_INTERVAL == 0:\n",
    "            trial.report(np.mean(scores), step=i)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "    return np.mean(scores) if scores else 0.0\n",
    "\n",
    "# Create study\n",
    "sampler = TPESampler(seed=RANDOM_SEED)\n",
    "pruner = MedianPruner(n_startup_trials=10, n_warmup_steps=0, interval_steps=1)\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    sampler=sampler,\n",
    "    pruner=pruner,\n",
    "    study_name=\"LSA_HDBSCAN_Tuned\"\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(objective, n_trials=60, show_progress_bar=True)\n",
    "\n",
    "# Get best results\n",
    "best_params = study.best_params\n",
    "best_score = float(study.best_value)\n",
    "\n",
    "print(\"Best Params:\")\n",
    "print(json.dumps(best_params, indent=2))\n",
    "print(\"Best Custom Score:\", round(best_score, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae7ebe",
   "metadata": {},
   "source": [
    "####  Best Params: { \"n_components\": 24, \"min_cluster_size\": 7, \"min_samples\": 1, \"top_k_ratio\": 0.25, \"pos_bias\": 0.00}\n",
    "#### Best Custom Score: 0.4029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce649f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
