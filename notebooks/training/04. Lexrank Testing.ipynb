{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "476eed60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports done.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Imports done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7ca411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy model loaded.\n"
     ]
    }
   ],
   "source": [
    "experiential_terms = {\n",
    "    'emotional': [\n",
    "        'felt','feeling','emotion','joy','fear','anxiety','bliss','love','terror','peace','calm',\n",
    "        'excited','overwhelmed','gratitude','euphoria','sadness','longing','crying','ecstasy','relief',\n",
    "        'compassion','grief','awe','anger','release','hope','despair','serenity','agitation','comfort',\n",
    "        'purging','vulnerability','intimacy','empathy','tension','melancholy','abandon','appreciation'\n",
    "    ],\n",
    "    'sensory': [\n",
    "        'visual','hear','sound','color','bright','pattern','geometry','music','taste','smell','see',\n",
    "        'saw','colors','sounds','shapes','textures','movement','melting','vibrations','pulsing',\n",
    "        'fractal','echo','flashing','tunnel','fluid','shimmering','sparkling','synesthesia',\n",
    "        'auditory','trails','glow','hallucination','pulsate','distortion','radiance','static',\n",
    "        'blurred','lightness','glimmer','resonance','tactile','kaleidoscopic'\n",
    "    ],\n",
    "    'cognitive': [\n",
    "        'thought','mind','consciousness','aware','realize','understand','insight','clarity','confused',\n",
    "        'clear','thinking','perception','concepts','identity','ego','dissolve','looping','logic',\n",
    "        'recognition','belief','interpretation','memory','language','narrative','meaning','mindspace',\n",
    "        'headspace','overthinking','mental','clarification','self-talk','rational','intellect',\n",
    "        'philosophical','metacognition','rumination','stream of consciousness','inner dialogue',\n",
    "        'cognitive dissonance','hyperfocus'\n",
    "    ],\n",
    "    'physical': [\n",
    "        'body','skin','breath','heart','energy','vibration','tingling','warm','heavy','light','pressure',\n",
    "        'sensation','nausea','shaking','sweating','floating','stillness','tightness','spasm','motion',\n",
    "        'trembling','cold','breathing','heartbeat','twitching','dry mouth','muscles','stiffness',\n",
    "        'paralysis','numbness','restlessness','chills','sweat','clenching','somatic','bodyload',\n",
    "        'temperature','digestive','physical release'\n",
    "    ],\n",
    "    'mystical': [\n",
    "        'ego','self','unity','divine','spiritual','transcend','infinite','oneness','god','universe',\n",
    "        'connected','sacred','eternal','death','rebirth','timeless','interconnected','presence','source',\n",
    "        'void','light','beyond','higher power','awakening','realm','dimension','truth','immortality',\n",
    "        'ego death','no-self','nirvana','cosmic','transcendence','pure being','karma','light being',\n",
    "        'soul','heaven','angelic','time distortion','godlike','divinity','portal','third eye',\n",
    "        'nondual','dissolution','samsara','infinity','entity','timelessness'\n",
    "    ],\n",
    "    'temporal': [\n",
    "        'onset','peak','comedown','duration','timeline','hours','minutes','start','beginning',\n",
    "        'after','later','build-up','before','end','wave','early','gradual','suddenly',\n",
    "        'phase','stage','passed','elapsed','over time','rush','fade','linger','moment',\n",
    "        'slowly','time passed','time distorted','hour mark','entry','exit'\n",
    "    ]\n",
    "}\n",
    "\n",
    "EXP_TERMS_FLAT = {word.lower() for words in experiential_terms.values() for word in words}\n",
    "\n",
    "# Load SpaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"lemmatizer\", \"ner\"])\n",
    "    if \"senter\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe(\"sentencizer\")\n",
    "    print(\"SpaCy model loaded.\")\n",
    "except OSError:\n",
    "    raise RuntimeError(\"SpaCy model not found. Run: python -m spacy download en_core_web_sm\")\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40e495da",
   "metadata": {},
   "outputs": [],
   "source": [
    "_whitespace_re = re.compile(r\"[ \\t\\v\\f]+\")\n",
    "_newlines_re   = re.compile(r\"\\s*\\n\\s*\")\n",
    "\n",
    "def clean_text_basic(txt: str) -> str:\n",
    "    if not isinstance(txt, str) or pd.isna(txt):\n",
    "        return \"\"\n",
    "    txt = txt.lower()\n",
    "    txt = txt.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    txt = _newlines_re.sub(\"\\n\", txt)\n",
    "    txt = _whitespace_re.sub(\" \", txt).strip()\n",
    "    return txt\n",
    "\n",
    "def sentencize_and_clean(raw_text: str, nlp, remove_sw_for_graph=True, remove_sw_for_summary=False):\n",
    "    raw_text_clean = clean_text_basic(raw_text)\n",
    "    doc = nlp.make_doc(raw_text_clean)\n",
    "    if \"senter\" in nlp.pipe_names:\n",
    "        nlp.get_pipe(\"senter\")(doc)\n",
    "    else:\n",
    "        doc = nlp(raw_text_clean)\n",
    "    sents = [s.text.strip() for s in doc.sents if s.text.strip()]\n",
    "\n",
    "    def remove_stopwords(s):\n",
    "        d = nlp.make_doc(s)\n",
    "        tokens = [t.text for t in d if t.is_alpha and t.text not in SPACY_STOPWORDS]\n",
    "        return \" \".join(tokens).strip()\n",
    "\n",
    "    s_graph = [remove_stopwords(s) for s in sents] if remove_sw_for_graph else sents\n",
    "    s_summary = sents if not remove_sw_for_summary else [remove_stopwords(s) for s in sents]\n",
    "\n",
    "    return sents, s_graph, s_summary, raw_text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c9256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _precompute_doc(text, nlp):\n",
    "    sents, s_graph, s_summary, cleaned = sentencize_and_clean(\n",
    "        text, nlp,\n",
    "        remove_sw_for_graph=True,\n",
    "        remove_sw_for_summary=False\n",
    "    )\n",
    "    if len(sents) == 0:\n",
    "        return [], np.zeros((0,0), dtype=np.float32), cleaned\n",
    "\n",
    "    vec = TfidfVectorizer(lowercase=False, stop_words=\"english\", max_features=4000)\n",
    "    X = vec.fit_transform(s_graph)\n",
    "    sim = cosine_similarity(X).astype(np.float32)\n",
    "    np.fill_diagonal(sim, 0.0)\n",
    "    return s_summary, sim, cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f77c68dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pagerank(P: np.ndarray, damping: float = 0.85, eps: float = 1e-6, max_iter: int = 100) -> np.ndarray:\n",
    "    n = P.shape[0]\n",
    "    if n == 0:\n",
    "        return np.array([])\n",
    "    v = np.ones(n, dtype=np.float32) / n\n",
    "    teleport = np.ones(n, dtype=np.float32) / n\n",
    "    for _ in range(max_iter):\n",
    "        v_new = damping * P.T.dot(v) + (1 - damping) * teleport\n",
    "        if np.linalg.norm(v_new - v, ord=1) < eps:\n",
    "            return v_new\n",
    "        v = v_new\n",
    "    return v\n",
    "\n",
    "def _summary_from_cache(entry, compression_ratio: float, similarity_threshold: float, damping_factor: float):\n",
    "    s_render = entry[\"s_render\"]\n",
    "    sim = entry[\"sim\"]\n",
    "    if len(s_render) == 0 or sim.size == 0:\n",
    "        return \"\"\n",
    "\n",
    "    A = np.where(sim >= similarity_threshold, sim, 0.0).astype(np.float32)\n",
    "    row_sums = A.sum(axis=1, keepdims=True)\n",
    "    row_sums[row_sums == 0] = 1.0\n",
    "    P = A / row_sums\n",
    "\n",
    "    scores = _pagerank(P, damping=damping_factor)\n",
    "    k = max(1, int(round(len(s_render) * compression_ratio)))\n",
    "    top_idx = np.argsort(-scores)[:k]\n",
    "    return \" \".join(s_render[i] for i in sorted(top_idx)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "729ab6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tfidf_cosine(a: str, b: str) -> float:\n",
    "    if not a.strip() or not b.strip():\n",
    "        return 0.0\n",
    "    vec = TfidfVectorizer(lowercase=False, stop_words=\"english\", max_features=4000)\n",
    "    try:\n",
    "        X = vec.fit_transform([a, b])\n",
    "        return float(cosine_similarity(X[0], X[1])[0,0])\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def score_semantic(summary: str, document: str) -> float:\n",
    "    return _tfidf_cosine(summary, document)\n",
    "\n",
    "def score_experiential(summary: str, document: str) -> float:\n",
    "    if not summary or not document:\n",
    "        return 0.0\n",
    "    doc_terms = {w for w in EXP_TERMS_FLAT if w in document}\n",
    "    if not doc_terms:\n",
    "        return 1.0\n",
    "    sum_terms = {w for w in EXP_TERMS_FLAT if w in summary}\n",
    "    return len(doc_terms & sum_terms) / len(doc_terms)\n",
    "\n",
    "def score_coherence(summary: str) -> float:\n",
    "    if not summary.strip():\n",
    "        return 0.0\n",
    "    sents = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', summary) if s.strip()]\n",
    "    if len(sents) < 2:\n",
    "        return 1.0\n",
    "    vec = TfidfVectorizer(lowercase=False, stop_words=\"english\", max_features=4000)\n",
    "    try:\n",
    "        X = vec.fit_transform(sents)\n",
    "        sims = [float(cosine_similarity(X[i], X[i+1])[0,0]) for i in range(len(sents)-1)]\n",
    "        return float(np.mean(sims)) if sims else 0.0\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def custom_score(summary: str, document: str) -> float:\n",
    "    if not summary or not document:\n",
    "        return 0.0\n",
    "    sem = score_semantic(summary, document)\n",
    "    exp = score_experiential(summary, document)\n",
    "    coh = score_coherence(summary)\n",
    "    return 0.5*sem + 0.3*exp + 0.2*coh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87bcde7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {\n",
    "    'compression_ratio': 0.3,\n",
    "    'similarity_threshold': 0.15,\n",
    "    'damping_factor': 0.85\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027a8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Processing DMT...\n",
      "Loaded 100 reports for DMT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing DMT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:04<00:00, 21.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved summaries to D:\\GitHub\\Psychedelics Summary\\Dataset\\Train Test\\dmt_test_100_with_summary.csv\n",
      "âœ”ï¸ DMT - Avg Final Score: 0.443\n",
      "\n",
      "ðŸ“Š Processing LSD...\n",
      "Loaded 100 reports for LSD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing LSD: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:04<00:00, 24.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved summaries to D:\\GitHub\\Psychedelics Summary\\Dataset\\Train Test\\lsd_test_100_with_summary.csv\n",
      "âœ”ï¸ LSD - Avg Final Score: 0.461\n",
      "\n",
      "ðŸ“Š Processing Psilocybin...\n",
      "Loaded 100 reports for Psilocybin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing Psilocybin: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:04<00:00, 21.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved summaries to D:\\GitHub\\Psychedelics Summary\\Dataset\\Train Test\\mushroom_test_100_with_summary.csv\n",
      "âœ”ï¸ Psilocybin - Avg Final Score: 0.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(r\"\")\n",
    "test_files = {\n",
    "    \"DMT\": DATA_DIR / \"dmt_test_100.csv\",\n",
    "    \"LSD\": DATA_DIR / \"lsd_test_100.csv\",\n",
    "    \"Psilocybin\": DATA_DIR / \"mushroom_test_100.csv\"\n",
    "}\n",
    "\n",
    "# Normalize substance names\n",
    "def _norm_sub(x):\n",
    "    if not isinstance(x, str): return \"OTHER\"\n",
    "    y = x.strip().upper()\n",
    "    if y in {\"DMT\"}: return \"DMT\"\n",
    "    if y in {\"LSD\", \"ACID\"}: return \"LSD\"\n",
    "    if y in {\"PSILOCYBIN\", \"PSILOCYBIN MUSHROOM\", \"MUSHROOM\", \"MUSHROOMS\", \"PSILOCYBE\"}:\n",
    "        return \"Psilocybin\"\n",
    "    return y\n",
    "\n",
    "results = []\n",
    "\n",
    "for substance, file_path in test_files.items():\n",
    "    print(f\"\\n Processing {substance}...\")\n",
    "    if not file_path.exists():\n",
    "        print(f\" File not found: {file_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    if \"report_text\" not in df.columns:\n",
    "        raise KeyError(f\"Missing 'report_text' in {file_path}\")\n",
    "\n",
    "    df[\"_subst_norm\"] = df[\"substance\"].map(_norm_sub)\n",
    "    df = df[df[\"_subst_norm\"] == substance].copy()\n",
    "    print(f\"Loaded {len(df)} reports for {substance}\")\n",
    "\n",
    "    # Precompute cache\n",
    "    cache = []\n",
    "    for text in df[\"report_text\"].astype(str).fillna(\"\").tolist():\n",
    "        s_render, sim, cleaned = _precompute_doc(text, nlp)\n",
    "        cache.append({\n",
    "            \"s_render\": s_render,\n",
    "            \"sim\": sim,\n",
    "            \"doc_clean\": cleaned\n",
    "        })\n",
    "\n",
    "    # Generate summaries\n",
    "    summaries = []\n",
    "    semantic_scores = []\n",
    "    experiential_scores = []\n",
    "    coherence_scores = []\n",
    "    final_scores = []\n",
    "\n",
    "    for entry in tqdm(cache, desc=f\"Summarizing {substance}\", total=len(cache)):\n",
    "        summary = _summary_from_cache(entry, **best_params)\n",
    "        summaries.append(summary)\n",
    "\n",
    "        sem = score_semantic(summary, entry[\"doc_clean\"])\n",
    "        exp = score_experiential(summary, entry[\"doc_clean\"])\n",
    "        coh = score_coherence(summary)\n",
    "        final = custom_score(summary, entry[\"doc_clean\"])\n",
    "\n",
    "        semantic_scores.append(sem)\n",
    "        experiential_scores.append(exp)\n",
    "        coherence_scores.append(coh)\n",
    "        final_scores.append(final)\n",
    "\n",
    "    # Add summary to df\n",
    "    df[\"summary\"] = summaries\n",
    "\n",
    "    # Save updated df\n",
    "    output_path = file_path.with_name(file_path.stem + \"_with_summary.csv\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Saved summaries to {output_path}\")\n",
    "\n",
    "    # Aggregate scores\n",
    "    avg_sem = np.mean(semantic_scores)\n",
    "    avg_exp = np.mean(experiential_scores)\n",
    "    avg_coh = np.mean(coherence_scores)\n",
    "    avg_final = np.mean(final_scores)\n",
    "\n",
    "    results.append({\n",
    "        \"Model\": \"M1\",  # LexRank\n",
    "        \"Substance\": substance,\n",
    "        \"Semantic (TF-IDF/SBERT)\": f\"{avg_sem:.2f} (TF-IDF)\",\n",
    "        \"Experiential\": f\"{avg_exp:.2f}\",\n",
    "        \"Coherence (TF-IDF/SBERT)\": f\"{avg_coh:.2f} (TF-IDF)\",\n",
    "        \"Final Score\": f\"{avg_final:.2f}\"\n",
    "    })\n",
    "\n",
    "    print(f\"âœ”ï¸ {substance} - Avg Final Score: {avg_final:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f616866e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " FINAL RESULTS TABLE (FOR REPORTING)\n",
      "============================================================\n",
      "Model  Substance Semantic (TF-IDF/SBERT) Experiential Coherence (TF-IDF/SBERT) Final Score\n",
      "   M1        DMT           0.63 (TF-IDF)         0.37            0.09 (TF-IDF)        0.44\n",
      "   M1        LSD           0.65 (TF-IDF)         0.40            0.08 (TF-IDF)        0.46\n",
      "   M1 Psilocybin           0.66 (TF-IDF)         0.41            0.08 (TF-IDF)        0.47\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" FINAL RESULTS TABLE (FOR REPORTING)\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02a78726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Results saved to D:\\GitHub\\Psychedelics Summary\\Dataset\\Train Test\\lexrank_test_scores.csv\n"
     ]
    }
   ],
   "source": [
    "results_df.to_csv(DATA_DIR / \"lexrank_test_scores.csv\", index=False)\n",
    "print(f\"\\n Results saved to {DATA_DIR / 'lexrank_test_scores.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d573228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
